{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PaliGemma for deployment\n",
    "\n",
    "Run the cells below and follow the instructions to deploy the model to the endpoint. You should set the PROCESSING_DIR variable to a directory on your machine which is not git-tracked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING_DIR = \"./TEMPS\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {PROCESSING_DIR}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/requirements.txt\n",
    "accelerate\n",
    "bitsandbytes\n",
    "git+https://github.com/huggingface/transformers.git@v4.41.2\n",
    "Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/inference.py\n",
    "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
    "import torch\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_RGB_image_io(image_bytes):\n",
    "    image_io = BytesIO(image_bytes)\n",
    "    image = Image.open(image_io)\n",
    "    resized_image = image.convert(\"RGB\")\n",
    "    return resized_image\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(model_dir, torch_dtype=torch.bfloat16).to(device)\n",
    "    # Load the processor\n",
    "    processor = PaliGemmaProcessor.from_pretrained(model_dir)\n",
    "    return [model, processor]\n",
    "\n",
    "\n",
    "def predict_fn(data, model_process):\n",
    "    model = model_process[0]\n",
    "    processor = model_process[1]\n",
    "    \n",
    "    # get prompt & image\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    image_b64=data.get(\"image\", \"\")\n",
    "    \n",
    "    # decode image from Base64\n",
    "    image_data = base64.b64decode(image_b64)\n",
    "    input_image = get_RGB_image_io(image_data)\n",
    "\n",
    "    inputs = processor(text=prompt, images=input_image, padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(device)\n",
    "    inputs = inputs.to(dtype=model.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=496)\n",
    "        str_out = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return {\"response\": str_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the model\n",
    "\n",
    "Alternatively to the following code, you can download the model by yourself (using Huggingface CLI, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copytree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "HF_MODEL_ID=\"google/paligemma-3b-mix-224\"\n",
    "# you need to accept the Gemma terms and conditions at: https://huggingface.co/google/paligemma-3b-mix-224\n",
    "HF_TOKEN=input(\"Please fill in your HuggingFace token: \")\n",
    "# \"Please set HF_TOKEN to your huggingface token. You can find it here: https://huggingface.co/settings/tokens\"\n",
    "assert len(HF_TOKEN) > 0\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(\n",
    "    repo_id=HF_MODEL_ID,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    local_dir=f\"{PROCESSING_DIR}/hf_download\"\n",
    "    )\n",
    "\n",
    "# create model dir\n",
    "model_folder_name=f\"model-{random.getrandbits(16)}\"\n",
    "model_tar = Path(PROCESSING_DIR, model_folder_name)\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copytree(snapshot_dir, str(model_tar), dirs_exist_ok=True)\n",
    "\n",
    "# copy code/ to model dir\n",
    "copytree(f\"{PROCESSING_DIR}/code/\", str(model_tar.joinpath(\"code\")), dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {PROCESSING_DIR}; tar cvf model.tar.gz -C ./{model_folder_name} ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
